# -*- coding: utf-8 -*-
"""Notebook_creation_modele_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tOZrK_sP177mLnU4CLSXXtjj_TfHfDGE

### module Python
"""

import os
import tensorflow as tf
import numpy as np
import pickle
from tensorflow import keras
#import matplotlib.pyplot as plt
#import shutil

"""### fonctions utiles"""

def plot_history(history, epochs) :
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    loss = history.history['loss']
    val_loss = history.history['val_loss']

    epochs_range = range(epochs)

    plt.figure(figsize=(8, 8))
    plt.subplot(1, 2, 1)

    plt.plot(epochs_range, acc, label='Training Accuracy')
    plt.plot(epochs_range, val_acc, label='Validation Accuracy')
    plt.legend()
    plt.title('Training and Validation Accuracy')

    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, loss, label='Training Loss')
    plt.plot(epochs_range, val_loss, label='Validation Loss')
    plt.legend()
    plt.title('Training and Validation Loss')
    plt.show()

def load_data(DIR, height=200, width=200, batch_size=128):
    train_ds = keras.utils.image_dataset_from_directory(DIR,\
                                                        labels='inferred' ,# labels générés depuis les sous dossier
                                                        label_mode = 'categorical', #créer un vecteur de taille n_class
                                                        color_mode = "rgb", #image de dimension L*H*3
                                                        image_size = (height, width), # rescaling des images
                                                        validation_split=0.2, # %validation
                                                        batch_size=batch_size,
                                                        subset = 'training', #retourne le training set
                                                        seed=43
                                                        )
    test_ds = keras.utils.image_dataset_from_directory(DIR,\
                                                       labels='inferred' ,# labels générés depuis les sous dossier
                                                       label_mode = 'categorical', #créer un vecteur de taille n_class
                                                   color_mode = "rgb", #image de dimension L*H*3
                                                   image_size = (height, width), # rescaling des images
                                                   validation_split=0.2, # %validation
                                                   subset = 'validation', #retourne le training set
                                                   batch_size=batch_size,
                                                   seed=43
                                            )
    return train_ds, test_ds

def split_test_val(ds, prct=0.2) :
    n = int(np.ceil(1/prct))
    print(n)
    val_batches = tf.data.experimental.cardinality(ds)
    test_ds = ds.take(val_batches // n)
    val_ds = ds.skip(val_batches // n)
    return val_ds, test_ds

"""### Création du DataSet"""

"""
NB = "Colab"
#NB = "jupyter"

if NB == "Colab" :
    IDIR = "/content/drive/Othercomputers/Mon ordinateur portable/P6_classer_image/data/Images"
    from google.colab import drive
    drive.mount('/content/drive')
    DIR = "/data_final/"
else :
    IDIR = "data/Images"
    DIR = "./tmp_final/"
if not os.path.isdir(DIR+"/Images"):
    os.makedirs(DIR+"/Images")
list_class = os.listdir(IDIR)[:]
print("Races de chiens :")
print(list_class)
for class_name in list_class :
    shutil.copytree(IDIR+"/"+class_name, DIR+"/Images/"+class_name, symlinks=False, ignore=None)
"""
DIR = "data/"
train_ds, val_ds = load_data(DIR+"/Images", batch_size=32, height=160, width=160)
val_ds, test_ds = split_test_val(val_ds, prct=0.2)

"""
tf.data.experimental.save(test_ds, 'test_ds')
with open('test_ds' + '/element_spec', 'wb') as out_:  
    pickle.dump(test_ds.element_spec, out_)
"""

print(f'Number of training batches: {tf.data.experimental.cardinality(train_ds)}')
print(f'Number of validation batches: {tf.data.experimental.cardinality(val_ds)}')
print(f'Number of test batches: {tf.data.experimental.cardinality(test_ds)}')

train_ds.class_names = list(map(lambda x : "-".join(x.split("-")[1:]), train_ds.class_names))
class_names = train_ds.class_names
with open('class_names.pkl', 'wb') as f :
    pickle.dump(class_names, f)
#val_ds.class_names = list(map(lambda x : "-".join(x.split("-")[1:]), val_ds.class_names))
#test_ds.class_names = list(map(lambda x : "-".join(x.split("-")[1:]), test_ds.class_names))

AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)
test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)

"""### création du modèle"""

data_augmentation = tf.keras.Sequential([
  tf.keras.layers.RandomFlip('horizontal'),
    tf.keras.layers.RandomRotation(0.2),
    tf.keras.layers.RandomZoom(0.2)
])

preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input

base_model = tf.keras.applications.MobileNetV2(input_shape=(160,160,3),
                                               include_top=False,
                                               weights='imagenet')

global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
prediction_layer = tf.keras.layers.Dense(len(class_names), activation='softmax')

inputs = tf.keras.Input(shape=(160, 160, 3))
x = data_augmentation(inputs)
x = preprocess_input(x)
x = base_model(x, training=False)
x = global_average_layer(x)
x = tf.keras.layers.Dropout(0.2)(x)

outputs = prediction_layer(x)
model = tf.keras.Model(inputs, outputs)

"""### apprentissage couche de prédiction"""

for layer in base_model.layers :
    layer.trainable = False

base_learning_rate = 0.0001
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),
              loss='categorical_crossentropy',
              metrics=['accuracy'])
model.summary()


initial_epochs = 30
checkpoint_filepath = 'weights_last_dense.{epoch:02d}-{val_accuracy:.4f}.h5'
model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)

history = model.fit(train_ds,
                    epochs=initial_epochs,
                    validation_data=val_ds,
                    callbacks=[model_checkpoint_callback])

history_dict = history.history

with open('history.pkl', 'wb') as ofile :
    pickle.dump(history_dict, ofile)
model.save("model_last_layer.h5")
#plot_history(history,30)

"""### fine tuning"""
base_model.trainable = True 
for layer in base_model.layers[:120]:
  layer.trainable = False

model.compile(loss='categorical_crossentropy',
              optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),
              metrics=['accuracy'])

model.summary()

fine_tune_epochs = 30
total_epochs =  initial_epochs + fine_tune_epochs

checkpoint_filepath = 'weights_fine_tuning.{epoch:02d}-{val_accuracy:.4f}.h5'
model_checkpoint_callback = keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)

history_fine = model.fit(train_ds,
                         epochs=total_epochs,
                         initial_epoch=history.epoch[-1],
                         validation_data=val_ds,
                         callbacks=[model_checkpoint_callback])

history_fine_dict = history_fine.history

with open('history_fine.pkl', 'wb') as ofile :
    pickle.dump(history_fine_dict, ofile)
model.save("model_fine_tune.h5")

#plot_history(history_fine,30)
